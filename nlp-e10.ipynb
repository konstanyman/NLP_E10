{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs\n",
    "# !pip install --quiet sentence-splitter regex\n",
    "# !pip install --quiet gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import wget\n",
    "import os\n",
    "from sentence_splitter import SentenceSplitter\n",
    "import regex\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### functions ###\n",
    "\n",
    "# load file\n",
    "def download_file(url, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        wget.download(url, filename)\n",
    "    else:\n",
    "        print(f\"{filename} already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiwiki-sample.txt already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "download_file('http://dl.turkunlp.org/TKO_7095_2023/fiwiki-20221120-sample.txt', 'fiwiki-sample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split into sentences\n",
    "\n",
    "# first into paragraphs\n",
    "with open('fiwiki-sample.txt', 'r', encoding='utf-8') as file:\n",
    "    paragraphs = file.readlines()\n",
    "\n",
    "# initialize splitter and split first 10 000 paragraphs into sentences\n",
    "splitter = SentenceSplitter(language='fi') \n",
    "sentences = [s for p in paragraphs[:100000] for s in splitter.split(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Patrick', 'Joseph', 'Leahy', '(', 's', '.']\n",
      "['31', '.', 'maaliskuuta', '1940', 'Montpelier', ',', 'Vermont', ')', 'on', 'yhdysvaltalainen', 'demokraattisen', 'puolueen', 'poliitikko', '.']\n",
      "['Leahy', 'toimii', 'Yhdysvaltain', 'senaatin', 'president', 'pro', 'temporena', 'eli', 'de', 'facto', 'senaatin', 'varapresidenttinä', '.']\n",
      "['Hän', 'on', 'toiminut', 'Vermontin', 'osavaltion', 'senaattorina', 'vuodesta', '1975', '.']\n",
      "['Grassley', 'myös', 'toimi', 'senaatin', 'president', 'pro', 'temporena', 'joulukuusta', '2012', 'tammikuuhun', '2015', '.']\n",
      "['Hän', 'on', 'ollut', 'myös', 'senaatin', 'oikeusvaliokunnan', 'puheenjohtaja', '.']\n",
      "['Elävä', 'kuollut', 'eli', 'epäkuollut', 'tarkoittaa', 'yleisesti', 'erilaisia', 'taruolentoja', ',', 'jotka', 'ovat', 'heränneet', 'kuolleista', 'takaisin', 'elävien', 'maailmaan', '.']\n",
      "['Populaarikulttuurissa', 'tunnetuimpia', 'eläviä', 'kuolleita', 'ovat', 'vampyyrit', 'ja', 'zombit', '.']\n",
      "['Sanan', \"'\", 'epäkuollut', \"'\", 'kehitti', 'kääntäjä', 'Kersti', 'Juva', 'englannin', 'sanan', '\"', 'undead', '\"', 'vastineeksi', 'kääntäessään', 'J', '.', 'R', '.', 'R', '.', 'Tolkienin', '\"', 'Tarua', 'sormusten', 'herrasta', '\"', '.']\n",
      "['Isku', 'pelasi', 'ensimmäisen', 'kautensa', 'mestaruussarjassa', 'vuonna', '1962', ',', 'kun', 'vuoden', '1961', 'Suomen', '-', 'mestari', 'Järvensivun', 'Kisa', 'luopui', 'sarjapaikastaan', 'Iskun', 'hyväksi', '.']\n"
     ]
    }
   ],
   "source": [
    "### split into tokens\n",
    "\n",
    "# sequence of alphanumeric characters followed by a space\n",
    "TOKENIZE_RE = regex.compile(r'([[:alnum:]]+|\\S)')\n",
    "\n",
    "# tokenize sentences\n",
    "tokenized = [TOKENIZE_RE.findall(s) for s in sentences]\n",
    "\n",
    "# print first 10 tokenized sentences\n",
    "for t in tokenized[:10]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the sentences are split quite poorly, but that's okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build gensim word2vec model that creates embeddings for words\n",
    "\n",
    "model = gensim.models.Word2Vec(\n",
    "    tokenized,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words:\n",
      "\n",
      "hyvä:\n",
      "\n",
      "['vahva', 'vaikeaa', 'todella', 'selkeä', 'huono', 'voimakas', 'vaikea', 'todellinen', 'älykäs', 'epäselvää']\n",
      "\n",
      " ------- \n",
      "\n",
      "huono:\n",
      "\n",
      "['selvä', 'heikko', 'hankala', 'vahva', 'selkeä', 'tarkka', 'nopea', 'epävarma', 'riippuvainen', 'tehokas']\n",
      "\n",
      " ------- \n",
      "\n",
      "koira:\n",
      "\n",
      "['sairaus', 'ihmiselle', 'paha', 'tauti', 'prosessi', 'totuus', 'helppo', 'ominaisuus', 'laite', 'teoria']\n",
      "\n",
      " ------- \n",
      "\n",
      "kissa:\n",
      "\n",
      "['näköinen', 'tekstissä', 'puuhun', 'hevonen', 'menneisyyden', 'kuvaan', 'abstrakti', 'paha', 'voimana', 'tuoksu']\n",
      "\n",
      " ------- \n",
      "\n",
      "kuningas:\n",
      "\n",
      "['keisari', 'Kaarle', 'Aleksanteri', 'kuninkaan', 'Ludvig', 'kuningatar', 'paavi', 'kuninkaaksi', 'Paavali', 'III']\n",
      "\n",
      " ------- \n",
      "\n",
      "kuningatar:\n",
      "\n",
      "['prinssi', 'Juhana', 'Eerik', 'Aleksanteri', 'Katariina', 'Vilhelm', 'Elisabet', 'Fredrik', 'Nikolai', 'Annan']\n",
      "\n",
      " ------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find most similar words\n",
    "\n",
    "words = ['hyvä', 'huono', 'koira', 'kissa', 'kuningas', 'kuningatar']\n",
    "\n",
    "print(\"Most similar words:\\n\")\n",
    "for word in words:\n",
    "    print(f\"{word}:\\n\\n{[x[0] for x in model.wv.most_similar(word, topn=10)]}\\n\\n ------- \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
